---
---
---

# Prediction Assignment

Author: Sérgio Paro

## Background

Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement -- a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how *much* of a particular activity they do, but they rarely quantify *how well they do it*. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

## **Data**

The training data for this project are available here:

[[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]{.underline}](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available here:

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

The data for this project come from this source: <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har>. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

## **Goal**

One thing that people regularly do is quantify how *much* of a particular activity they do, but they rarely quantify *how well they do it*.

In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

## **Data Acquisition and Manipulation**

The first part of the project is to load the data from the training and testing data files. For the training data set the following manipulations will be done:

-   Find and Eliminate from the training data set near zero variance predictors

-   Eliminate from the training data set useless predictors for the goal of this project (X, user_name, cvtd_timestamp, raw_timestamp_part_1, raw_timestamp_part_2, new_window, num_window)

```{r include=FALSE}
library(ggplot2)
library(caret)
library(rattle)
library(randomForest)
library(reshape) 

set.seed(1675)

## open csv files
pml_training <- read.csv("C:\\Users\\paros\\Documents\\Git\\data_science_public\\Prediction Assignment Writeup\\pml-training.csv", header=TRUE, stringsAsFactors=FALSE)
pml_testing <- read.csv("C:\\Users\\paros\\Documents\\Git\\data_science_public\\Prediction Assignment Writeup\\pml-testing.csv", header=TRUE, stringsAsFactors=FALSE)

## useless predictors X, user_name, cvtd_timestamp, raw_timestamp_part_1, raw_timestamp_part_2, new_window, num_window
useless_pred <- c(1:7)
pml_train_filt <- pml_training[, -useless_pred]

## find near zero variance predictors
nzv <- nearZeroVar(pml_train_filt, saveMetrics= TRUE)
nzv[nzv$nzv,]
nzv <- nearZeroVar(pml_train_filt)
## filter the training data set
pml_train_filt <- pml_train_filt[,-nzv]
## convert classe to factor
pml_train_filt$classe <- as.factor(pml_train_filt$classe)

## find NA predictors
isnotnacol = (colSums(is.na(pml_train_filt)) == 0) 
pml_train_filt <- pml_train_filt[, isnotnacol]
```

## **Model Training with Cross Validation**

The model to be trained is a random forest, known to be one of the models that delivery high accuracy, but, on the other hand, penalizing its interpretability.

Even though the random forest algorithm is already doing cross validation internally, the training data set will be split into two, one to be real training data set (`train_ds)` and the other(`val_ds`) that will be used to cross validated our model, before running it against the testing data set.

```{r include=FALSE}
inTrain <- createDataPartition(y=pml_train_filt$classe, p=0.75, list=FALSE)
# subset pml_training_filt data to training
train_ds <- pml_train_filt[inTrain,]
# subset spam data (the rest) to validate
val_ds <- pml_train_filt[-inTrain,]

model_rf <- randomForest(classe ~ ., data=train_ds, importance = TRUE)
```

#### Accuracy

The model has a **excellent accuracy of 99.5%** (1-OOB estimated of error rate) reflected as well in the numbers of the confusion matrix.

```{r echo=TRUE}
## print model useful information
print(model_rf)
```

#### Errors according to the number of trees

The following chat shows how the OOB (out-of-bag) and classes errors decrease dramatically when the model reaches the number of 100 trees and after that the error seems to stabilize.

```{r echo=TRUE}
## plot errors along number of tress
temp_df <- data.frame(OOB=model_rf$err.rate[,1], A=model_rf$err.rate[,2], B=model_rf$err.rate[,3], C=model_rf$err.rate[,4], D=model_rf$err.rate[,5], E=model_rf$err.rate[,6])
temp_df$Tree_Qty <- as.numeric(row.names(temp_df))
err_rate_df <- melt(temp_df, id="Tree_Qty")

g <- ggplot(data = err_rate_df, aes(x = Tree_Qty, y = value, group = variable, color = variable)) 
g <- g + labs(title="OOB and Classes Error", x ="Number of Trees", y = "Error")
g <- g + geom_line()
g
```

#### Variable Importance (Mean Decrease Accuracy and Mean Decrease Gini)

Each variable's importance is assessed based on 2 criteria:

**MeanDecreaseAccuracy**: gives a rough estimate of the loss in the model accuracy when that particular variable is removed from the training set.

**MeanDecreaseGini:** gives a rough estimate of the loss in node purity (highest purity means that each tree node contains only elements of a single class) when that particular variable is removed from the train.

The folllowing 2 charts show the top 10 most important variables in terms of the **MeanDecreaseAccuracy** and **MeanDecreaseGini.**

```{r echo=FALSE}
## prepare data for ggplot in a data frame
imp_temp <- data.frame(importance(model_rf))
Variable <- row.names.data.frame(imp_temp)
MeanDecreaseAccuracy <- imp_temp$MeanDecreaseAccuracy
MeanDecreaseGini <- imp_temp$MeanDecreaseGini
imp_var_ds <- data.frame(Variable, MeanDecreaseAccuracy, MeanDecreaseGini)

## plot the variable importance according to Mean Decrease Accuracy
g <- ggplot(data = imp_var_ds[tail(order(imp_var_ds$MeanDecreaseAccuracy), 10), ], aes(x=MeanDecreaseAccuracy, y = reorder(Variable, +MeanDecreaseAccuracy)))
g <- g + labs(title="Top 10 Variable Importance (Mean Decrease Accuracy)", x ="Importance", y = "Variable")
g <- g + geom_bar(stat="identity", aes(fill=MeanDecreaseAccuracy)) + theme_light() + labs(fill = "Variable")
g
```

```{r echo=FALSE}
## plot the variable importance according to Mean Decrease Gini
g <- ggplot(data = imp_var_ds[tail(order(imp_var_ds$MeanDecreaseGini), 10), ], aes(x=MeanDecreaseGini, y = reorder(Variable, +MeanDecreaseGini)))
g <- g + labs(title="Top 10 Variable Importance (Mean Decrease Gini)", x ="Importance", y = "Variable")
g <- g + geom_bar(stat="identity", aes(fill=MeanDecreaseGini)) + theme_light() + labs(fill = "Variable")
g
```

#### Cross Validation and Out of Sample Error

Using the generated model to predict "classe" in the validation data set shown the model perform very well in a completely new data set, reaching a **99.5% accuracy** and an estimate of **0.5% Out of Sample Error.**

So, based on the accuracy of model in training and validation phases we are going to proceed with this model to predict the entries in the training data set.

```{r echo=TRUE}
## cross-validation
predict_rf <- predict(model_rf, val_ds)
confusionMatrix(predict_rf, val_ds$classe)
```

#### Predicting the testing data set

Following the "classe" predictions for the entries in the training data set.

```{r echo=TRUE}
## prediction for testing dataset
predict_testing <- predict(model_rf, pml_testing)
predict_testing
```
